{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning de Llama 2 pour l'analyse de sentiments\n\n**Pour ce tutoriel pratique sur le fine-tuning de Llama 2, nous allons traiter d'une analyse de sentiments sur des informations financi√®res et √©conomiques.**   \n\n#### Probl√©matique\nL'analyse de sentiments sur des informations financi√®res et √©conomiques est cruciale pour les entreprises. Elle fournit des informations pr√©cieuses sur les **tendances du march√©, la confiance des investisseurs et le comportement des consommateurs.** De plus, elle permet d'identifier les risques potentiels pour la r√©putation et aide √† √©valuer le sentiment des parties prenantes, des investisseurs et du grand public, facilitant ainsi la prise de d√©cisions d'investissement √©clair√©es.\n\nAvant d'aborder les aspects techniques du finetuning d'un LLM comme Llama 2, nous devons trouver l'ensemble de donn√©es ad√©quat pour d√©montrer les possibilit√©s et capacit√©s du finetuning.\n\n#### Dataset\nL'ensemble de donn√©es **FinancialPhraseBank** est une collection compl√®te qui capture les sentiments des titres de l'actualit√© financi√®re du point de vue d'un investisseur. Comprenant deux colonnes cl√©s, √† savoir **\"Sentiment\"** et **\"Titre de l'actualit√©\"**, l'ensemble de donn√©es classe efficacement les sentiments comme √©tant **soit n√©gatifs, soit neutres, soit positifs**.  \n\n[Consultez la carte du Dataset ici](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news)","metadata":{}},{"cell_type":"markdown","source":"## Biblioth√®ques & Installations üóÇ","metadata":{}},{"cell_type":"markdown","source":"\nLes biblioth√®ques sp√©cifiques n√©cessaires sont entre autres:\n\n* **accelerate:** biblioth√®que d'entra√Ænement distribu√© pour PyTorch par HuggingFace. Elle vous permet d'entra√Æner vos mod√®les sur plusieurs GPU ou CPU en parall√®le (configurations distribu√©es), ce qui peut acc√©l√©rer consid√©rablement l'entra√Ænement en pr√©sence de plusieurs GPU (nous ne l'utiliserons pas dans notre exemple).\n\n* **peft:** biblioth√®que Python par HuggingFace pour l'adaptation efficace des LLMs sans ajuster tous les param√®tres du mod√®le. Les m√©thodes PEFT n'ajustent qu'un petit nombre de param√®tres (suppl√©mentaires) du mod√®le, r√©duisant ainsi consid√©rablement les co√ªts computationnels et de stockage.   \n[**D√©tails PEFT:** Voir ressources de la S√©ance 7](https://github.com/ANYANTUDRE/Stage-IA-Selever-GO-AI-Corp/blob/main/02.%20Supports%20de%20Cours%20-%20Formations/07.%20S%C3%A9ance%207%20-%20M%C3%A9triques%20%26%20%20PEFTs%20-%20LoRA%20%26%20QLoRA/03.%20Parameter%20efficient%20Fine-tuning%20(PEFT).pdf)\n\n* **bitsandbytes:** wrapper l√©ger autour des fonctions personnalis√©es CUDA, en particulier les optimisateurs 8 bits, la multiplication matricielle, et les fonctions de quantification. Il permet d'ex√©cuter des mod√®les stock√©s en pr√©cision 4 bits : bien que bitsandbytes stocke les poids en 4 bits, le calcul se fait toujours en 16 ou 32 bits et ici, toute combinaison peut √™tre choisie (float16, bfloat16, float32, etc.).    \n[**D√©tails Quantization:** Voir ressources de la S√©ance 5](https://github.com/ANYANTUDRE/Stage-IA-Selever-GO-AI-Corp/blob/main/02.%20Supports%20de%20Cours%20-%20Formations/05.%20S%C3%A9ance%205%20-%20Quantization%20%26%20Rappels/02.%20Technique%20de%20Quantization.pdf)\n\n* **transformers:** biblioth√®que pour le traitement du langage naturel (NLP). Elle fournit un certain nombre de mod√®les pr√©-entra√Æn√©s pour des t√¢ches NLP telles que la classification de texte, les r√©ponses aux questions, et la traduction automatique.  \n[**D√©tails Transformers:** Voir ressources de la S√©ance 4](https://github.com/ANYANTUDRE/Stage-IA-Selever-GO-AI-Corp/tree/main/02.%20Supports%20de%20Cours%20-%20Formations/04.%20S%C3%A9ance%204%20-%20Utilisation%20des%20Transformers%20de%20Hugging%20Face)\n\n* **trl:** biblioth√®que compl√®te par HuggingFace fournissant un ensemble d'outils pour entra√Æner des mod√®les de langage de type transformer avec l'apprentissage par renforcement, depuis l'√©tape de fine-tuning supervis√© (SFT), l'√©tape de mod√©lisation de la r√©compense (RM) jusqu'√† l'√©tape d'optimisation de la politique proximale (PPO).   \n[**Documentation officielle**](https://huggingface.co/docs/trl/sft_trainer)","metadata":{}},{"cell_type":"code","source":"!pip install -q -U \"torch==2.1.2\" tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:11:08.699155Z","iopub.execute_input":"2024-07-07T11:11:08.699480Z","iopub.status.idle":"2024-07-07T11:13:20.767848Z","shell.execute_reply.started":"2024-07-07T11:11:08.699454Z","shell.execute_reply":"2024-07-07T11:13:20.766740Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.2 which is incompatible.\ntensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.17.0 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\"","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:13:20.769385Z","iopub.execute_input":"2024-07-07T11:13:20.769769Z","iopub.status.idle":"2024-07-07T11:13:52.976216Z","shell.execute_reply.started":"2024-07-07T11:13:20.769732Z","shell.execute_reply":"2024-07-07T11:13:52.974941Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:13:52.979452Z","iopub.execute_input":"2024-07-07T11:13:52.979780Z","iopub.status.idle":"2024-07-07T11:14:45.791629Z","shell.execute_reply.started":"2024-07-07T11:13:52.979753Z","shell.execute_reply":"2024-07-07T11:14:45.790403Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"La cellule suivante importe le module os et d√©finit deux variables d'environnement :\n* **CUDA_VISIBLE_DEVICES** : variable d'environnement indiquant √† PyTorch quels GPU utiliser. Dans ce cas, le code d√©finit la variable d'environnement √† 0, ce qui signifie que PyTorch utilisera le premier GPU.\n* **TOKENIZERS_PARALLELISM** : variable d'environnement indiquant √† la biblioth√®que Hugging Face Transformers s'il faut parall√©liser le processus de tokenisation. Dans ce cas, le code d√©finit la variable d'environnement √† false, ce qui signifie que le processus de tokenisation ne sera pas parall√©lis√©.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:14:45.792968Z","iopub.execute_input":"2024-07-07T11:14:45.793241Z","iopub.status.idle":"2024-07-07T11:14:45.798215Z","shell.execute_reply.started":"2024-07-07T11:14:45.793217Z","shell.execute_reply":"2024-07-07T11:14:45.797276Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") ### filtrer et ignorer les avertissements inutiles","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:14:45.799215Z","iopub.execute_input":"2024-07-07T11:14:45.799523Z","iopub.status.idle":"2024-07-07T11:14:45.813215Z","shell.execute_reply.started":"2024-07-07T11:14:45.799492Z","shell.execute_reply":"2024-07-07T11:14:45.812485Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer,setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-07T11:16:38.712379Z","iopub.execute_input":"2024-07-07T11:16:38.713170Z","iopub.status.idle":"2024-07-07T11:16:38.719051Z","shell.execute_reply.started":"2024-07-07T11:16:38.713139Z","shell.execute_reply":"2024-07-07T11:16:38.718139Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(f\"PyTorch version --> {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:16:42.132522Z","iopub.execute_input":"2024-07-07T11:16:42.132873Z","iopub.status.idle":"2024-07-07T11:16:42.137987Z","shell.execute_reply.started":"2024-07-07T11:16:42.132847Z","shell.execute_reply":"2024-07-07T11:16:42.137059Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"PyTorch version --> 2.1.2+cu121\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device --> {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:16:45.842189Z","iopub.execute_input":"2024-07-07T11:16:45.842560Z","iopub.status.idle":"2024-07-07T11:16:45.847793Z","shell.execute_reply.started":"2024-07-07T11:16:45.842531Z","shell.execute_reply":"2024-07-07T11:16:45.846834Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Device --> cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Pr√©paration des donn√©es üõ¢ & utilitaires ","metadata":{}},{"cell_type":"markdown","source":"Le code dans la cellule suivante effectue les √©tapes suivantes :\n\n1. Lit le jeu de donn√©es d'entr√©e √† partir du fichier all-data.csv, qui est un fichier CSV avec deux colonnes : **sentiment et texte.**\n2. Divise le jeu de donn√©es en ensembles d'entra√Ænement et de test, avec 300 √©chantillons dans chaque ensemble. La division est stratifi√©e par sentiment, de sorte que chaque ensemble contient un √©chantillon repr√©sentatif de sentiments positifs, neutres et n√©gatifs.\n3. M√©lange les donn√©es d'entra√Ænement dans un ordre reproductible (random_state=2024).\n4. Transforme les textes contenus dans les donn√©es d'entra√Ænement et de test en prompts √† utiliser par Llama : les prompts d'entra√Ænement contiennent la r√©ponse attendue avec laquelle nous voulons affiner le mod√®le.\n5. Les exemples r√©siduels qui ne sont pas dans l'entra√Ænement ou le test, √† des fins de reporting pendant l'entra√Ænement (mais qui ne seront pas utilis√©s pour l'arr√™t anticip√©), sont trait√©s comme des donn√©es d'√©valuation, qui sont √©chantillonn√©es avec r√©p√©tition afin d'avoir un √©chantillon de 50/50/50 (les instances n√©gatives √©tant tr√®s peu nombreuses, elles doivent donc √™tre r√©p√©t√©es).\n6. Les donn√©es d'entra√Ænement et d'√©valuation sont encapsul√©es par la classe de Hugging Face (https://huggingface.co/docs/datasets/index).\n\n**Cela pr√©pare en une seule cellule les jeux de donn√©es train_data, eval_data et test_data √† utiliser dans notre fine-tuning.**","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=2024)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Analyse le sentiment du titre de l'actualit√© inclus entre crochets, \n            d√©termine s'il est positif, neutre ou n√©gatif, et retourner la r√©ponse \n            sous forme d'√©tiquette de sentiment correspondante \"positive\", \"neutral\" ou \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyse le sentiment du titre de l'actualit√© inclus entre crochets, \n            d√©termine s'il est positif, neutre ou n√©gatif, et retourner la r√©ponse \n            sous forme d'√©tiquette de sentiment correspondante \"positive\", \"neutral\" ou \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:16:58.001113Z","iopub.execute_input":"2024-07-07T11:16:58.001564Z","iopub.status.idle":"2024-07-07T11:16:59.247571Z","shell.execute_reply.started":"2024-07-07T11:16:58.001533Z","shell.execute_reply":"2024-07-07T11:16:59.246800Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Ensuite, nous cr√©ons une fonction pour √©valuer les r√©sultats de notre mod√®le de sentiment affin√©. La fonction effectue les √©tapes suivantes :\n\n1. Associe les √©tiquettes de sentiment √† une repr√©sentation num√©rique, o√π **2 repr√©sente positif, 1 repr√©sente neutre, et 0 repr√©sente n√©gatif.**\n2. Calcule la pr√©cision du mod√®le sur les donn√©es de test.\n3. G√©n√®re un rapport de pr√©cision pour chaque √©tiquette de sentiment.\n4. G√©n√®re un rapport de classification pour le mod√®le.\n5. G√©n√®re une matrice de confusion pour le mod√®le.","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    ### pr√©cision\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Pr√©cision: {accuracy:.3f}')\n        \n    ### rapport de classification\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nRapport de classification:')\n    print(class_report)\n    \n    ### matrice de confusion\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nMatrice de Confusion:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:17:00.560022Z","iopub.execute_input":"2024-07-07T11:17:00.560914Z","iopub.status.idle":"2024-07-07T11:17:00.567610Z","shell.execute_reply.started":"2024-07-07T11:17:00.560880Z","shell.execute_reply":"2024-07-07T11:17:00.566758Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Tester Llama2 avant fine-tuning üë®üèæ‚Äçüíª\n\nEnsuite, nous devons nous occuper du mod√®le, qui est un 7b-hf (7 milliards de param√®tres, sans RLHF, au format compatible avec HuggingFace), en le chargeant √† partir des mod√®les Kaggle et en le quantifiant.\n\n#### Chargement et quantification du mod√®le :\n\n* D'abord, le code charge le mod√®le de langage Llama-2 depuis le Hub Hugging Face.\n* Ensuite, le code obtient le type de donn√©es float16 de la biblioth√®que torch. C'est le type de donn√©es qui sera utilis√© pour les calculs.\n* Ensuite, il cr√©e un objet BitsAndBytesConfig avec les param√®tres suivants :\n    1. load_in_4bit : charge les poids du mod√®le au format 4 bits.\n    2. bnb_4bit_quant_type : utilise le type de quantification \"nf4\". Le 4-bit NormalFloat (NF4) est un nouveau type de donn√©es th√©oriquement optimal pour les poids distribu√©s normalement.\n    3. bnb_4bit_compute_dtype : utilise le type de donn√©es float16 pour les calculs.\n    4. bnb_4bit_use_double_quant : pour la double quantification (r√©duit l'empreinte m√©moire moyenne en quantifiant √©galement les constantes de quantification et √©conomise 0,4 bits suppl√©mentaires par param√®tre).\n* Ensuite, le code cr√©e un objet AutoModelForCausalLM √† partir du mod√®le de langage pr√©-entra√Æn√© Llama-2, en utilisant l'objet BitsAndBytesConfig pour la quantification.\n* Apr√®s cela, le code d√©sactive la mise en cache pour le mod√®le.\n* Enfin, le code d√©finit la probabilit√© des tokens de pr√©-entra√Ænement √† 1.\n\n\n#### Chargement du tokenizer :\n\n* D'abord, le code charge le tokenizer pour le mod√®le de langage Llama-2.\n* Ensuite, il d√©finit le token de padding comme √©tant le token de fin de s√©quence (EOS).\n* Enfin, le code d√©finit le c√¥t√© de padding sur \"right\", ce qui signifie que les s√©quences d'entr√©e seront remplies du c√¥t√© droit. C'est crucial pour une direction de padding correcte (c'est ainsi que fonctionne Llama 2).","metadata":{}},{"cell_type":"code","source":"model_name = \"togethercomputer/LLaMA-2-7B-32K\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:17:46.916766Z","iopub.execute_input":"2024-07-07T11:17:46.917391Z","iopub.status.idle":"2024-07-07T11:19:24.245063Z","shell.execute_reply.started":"2024-07-07T11:17:46.917355Z","shell.execute_reply":"2024-07-07T11:19:24.244195Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0866e16f650549d2840caaf1362793e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00668824e0e8432db126bc92c6ad9498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db312a8cddfb451db55673fd7a2989ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80348e8a8a374db08dfef9b4e06af4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8042e183bb49798ee4d8236bda0f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b79b030f6004cc8897ca3a66f074113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf9390e9dc24d6a871cb2ceba98262b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed1c734ec814a96a8af80509b0635a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9b95661d304f32a915ca1f08fc01bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f395e6de076456d851da062098ca0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d40d282d20493482ef1e1d42b07b5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f38661440e436a8983d6c88fc34cf9"}},"metadata":{}}]},{"cell_type":"markdown","source":"Dans la cellule suivante, nous d√©finissons une fonction pour pr√©dire le sentiment d'un titre d'actualit√© en utilisant le mod√®le de langage Llama-2. La fonction prend trois arguments :\n\n- `test` : un DataFrame Pandas contenant les titres d'actualit√©s √† pr√©dire.\n- `model` : le mod√®le de langage pr√©-entra√Æn√© Llama-2.\n- `tokenizer` : le tokenizer pour le mod√®le de langage Llama-2.\n\nLa fonction fonctionne comme suit :\n\n1. Pour chaque titre d'actualit√© dans le DataFrame `test` :\n    * Cr√©er un prompt pour le mod√®le de langage, lui demandant d'analyser le sentiment du titre d'actualit√© et de renvoyer l'√©tiquette de sentiment correspondante.\n    * Utiliser la fonction `pipeline()` de la biblioth√®que Hugging Face Transformers pour g√©n√©rer du texte √† partir du mod√®le de langage, en utilisant le prompt.\n    * Extrayer l'√©tiquette de sentiment pr√©dite du texte g√©n√©r√©.\n    * Ajouter l'√©tiquette de sentiment pr√©dite √† la liste `y_pred`.\n2. Retournez la liste `y_pred`.","metadata":{}},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 1,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:19:24.246591Z","iopub.execute_input":"2024-07-07T11:19:24.246888Z","iopub.status.idle":"2024-07-07T11:19:24.254025Z","shell.execute_reply.started":"2024-07-07T11:19:24.246862Z","shell.execute_reply":"2024-07-07T11:19:24.253062Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"√Ä ce stade, nous sommes pr√™ts √† tester le mod√®le Llama 2 7b-hf et √† √©valuer ses performances sur notre probl√®me sans aucun ajustement fin. Cela nous permet d'obtenir des informations sur le mod√®le lui-m√™me et d'√©tablir une ligne de base.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:19:24.255213Z","iopub.execute_input":"2024-07-07T11:19:24.255601Z","iopub.status.idle":"2024-07-07T11:25:23.468758Z","shell.execute_reply.started":"2024-07-07T11:19:24.255567Z","shell.execute_reply":"2024-07-07T11:25:23.467707Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:59<00:00,  2.51it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dans la cellule suivante, nous √©valuons les r√©sultats. Il y a peu √† dire, les performances sont vraiment terribles car le mod√®le 7b-hf a tendance √† pr√©dire principalement un sentiment neutre et d√©tecte rarement les sentiments positifs ou n√©gatifs.","metadata":{}},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:25:23.471948Z","iopub.execute_input":"2024-07-07T11:25:23.472231Z","iopub.status.idle":"2024-07-07T11:25:23.493738Z","shell.execute_reply.started":"2024-07-07T11:25:23.472205Z","shell.execute_reply":"2024-07-07T11:25:23.492737Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Pr√©cision: 0.339\n\nRapport de classification:\n              precision    recall  f1-score   support\n\n           0       1.00      0.00      0.01       300\n           1       0.33      1.00      0.50       300\n           2       0.83      0.02      0.03       300\n\n    accuracy                           0.34       900\n   macro avg       0.72      0.34      0.18       900\nweighted avg       0.72      0.34      0.18       900\n\n\nMatrice de Confusion:\n[[  1 299   0]\n [  0 299   1]\n [  0 295   5]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine-tuning ü¶æ\n\nDans la cellule suivante, nous pr√©parons tout pour le fine-tuning. Nous configurons et initialisons un entra√Æneur **Simple Fine-tuning Trainer (SFTTrainer)** pour entra√Æner le LLM en utilisant la m√©thode d'**ajustement efficace des param√®tres (PEFT)**, ce qui devrait √©conomiser du temps car elle op√®re sur un nombre r√©duit de param√®tres par rapport √† la taille totale du mod√®le.  \n\nLa m√©thode PEFT se concentre sur le raffinement d'un ensemble limit√© de param√®tres suppl√©mentaires du mod√®le, tout en maintenant la majorit√© des param√®tres du LLM pr√©-entra√Æn√© fixes. Cela r√©duit consid√©rablement les co√ªts √† la fois computationnels et de stockage. De plus, cette strat√©gie adresse le d√©fi de l'oubli catastrophique, qui se produit souvent lors du fine-tuning complet des LLMs.  \n\n[**D√©tails PEFT:** Voir ressources de la S√©ance 7](https://github.com/ANYANTUDRE/Stage-IA-Selever-GO-AI-Corp/blob/main/02.%20Supports%20de%20Cours%20-%20Formations/07.%20S%C3%A9ance%207%20-%20M%C3%A9triques%20%26%20%20PEFTs%20-%20LoRA%20%26%20QLoRA/03.%20Parameter%20efficient%20Fine-tuning%20(PEFT).pdf)\n\n\n#### PEFTConfig :\n\nL'objet `peft_config` sp√©cifie les param√®tres pour PEFT. Voici quelques param√®tres importants :\n\n- `lora_alpha` : taux d'apprentissage pour les matrices de mise √† jour LoRA.\n- `lora_dropout` : probabilit√© de dropout pour les matrices de mise √† jour LoRA (permet d'√©viter l'overfitting)\n- `r` : rang des matrices de mise √† jour LoRA.\n- `bias` : type de biais √† utiliser. Les valeurs possibles sont none, additive et learned.\n- `task_type` : type de t√¢che pour laquelle le mod√®le est entra√Æn√©. Les valeurs possibles sont CAUSAL_LM et MASKED_LM.\n\n#### TrainingArguments :\n\nL'objet `training_arguments` sp√©cifie les param√®tres pour l'entra√Ænement du mod√®le. Voici quelques param√®tres importants :\n\n- `output_dir` : r√©pertoire o√π les journaux d'entra√Ænement et les checkpoints seront sauvegard√©s.\n- `num_train_epochs` : nombre d'√©poques pour entra√Æner le mod√®le.\n- `per_device_train_batch_size` : nombre d'√©chantillons dans chaque batch sur chaque dispositif (ici on utilise un seul GPU)\n- `gradient_accumulation_steps` : nombre de batches pour accumuler les gradients avant de mettre √† jour les param√®tres du mod√®le.\n- `optim` :optimiseur √† utiliser pour l'entra√Ænement du mod√®le.\n- `save_steps` : nombre d'√©tapes apr√®s lesquelles sauvegarder un checkpoint.\n- `logging_steps` : nombre d'√©tapes apr√®s lesquelles enregistrer les m√©triques d'entra√Ænement.\n- `learning_rate` : taux d'apprentissage pour l'optimiseur.\n- `weight_decay` : param√®tre de d√©gradation du poids pour l'optimiseur.\n- `fp16` : utilisation ou non de la pr√©cision de virgule flottante 16 bits.\n- `bf16` : utilisation ou non de la pr√©cision BFloat16.\n- `max_grad_norm` : norme maximale du gradient.\n- `max_steps` : nombre maximal d'√©tapes pour entra√Æner le mod√®le.\n- `warmup_ratio` : proportion des √©tapes d'entra√Ænement √† utiliser pour le r√©chauffement du taux d'apprentissage.\n- `group_by_length` : grouper ou non les √©chantillons d'entra√Ænement par longueur.\n- `lr_scheduler_type` : type de planificateur de taux d'apprentissage √† utiliser.\n- `report_to` : outils auxquels rapporter les m√©triques d'entra√Ænement.\n- `evaluation_strategy` : strat√©gie pour √©valuer le mod√®le pendant l'entra√Ænement.\n\n\n#### SFTTrainer :\n\nLe SFTTrainer est une classe d'entra√Æneur personnalis√©e de la biblioth√®que TRL. Il est utilis√© pour entra√Æner les LLMs (utilisant √©galement la m√©thode PEFT).\n\nL'objet SFTTrainer est initialis√© avec les arguments suivants :\n\n- `model` : mod√®le √† entra√Æner.\n- `train_dataset` : dataset d'entra√Ænement.\n- `eval_dataset` : dataset d'√©valuation.\n- `peft_config` : configuration PEFT.\n- `dataset_text_field` : nom du champ texte dans le dataset.\n- `tokenizer` : tokenizer √† utiliser.\n- `args` : arguments d'entra√Ænement.\n- `packing` : indique si les √©chantillons d'entra√Ænement doivent √™tre empaquet√©s.\n- `max_seq_length` : longueur maximale de la s√©quence.\n\nUne fois que l'objet SFTTrainer est initialis√©, il peut √™tre utilis√© pour entra√Æner le mod√®le en appelant la m√©thode `train()`.","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n        lora_alpha=32, \n        lora_dropout=0.1,\n        r=8,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"trained_weigths\",             ### r√©pertoire pour sauvegarder et identifiant du r√©f√©rentiel\n    num_train_epochs=1,                       ### nombre d'√©poques d'entra√Ænement\n    per_device_train_batch_size=1,            ### taille du batch par p√©riph√©rique pendant l'entra√Ænement\n    gradient_accumulation_steps=4,            ### nombre d'√©tapes avant d'effectuer une passe de r√©tropropagation/mise √† jour\n    gradient_checkpointing=True,              ### utilise le point de contr√¥le de gradient pour √©conomiser de la m√©moire\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,                         ### enregistre chaque 25 √©tapes\n    learning_rate=2e-4,                       ### taux d'apprentissage, bas√© sur le papier QLoRA\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        ### norme maximale du gradient bas√©e sur le papier QLoRA\n    max_steps=-1,\n    warmup_ratio=0.03,                        ### ratio de pr√©chauffage bas√© sur le papier QLoRA\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",               ### utilise le programmeur de taux d'apprentissage cosinus\n    report_to=\"tensorboard\",                  ### rapporte les m√©triques √† tensorboard\n    evaluation_strategy=\"epoch\"               ### sauvegarde le checkpoint √† chaque √©poque\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:25:23.494967Z","iopub.execute_input":"2024-07-07T11:25:23.495283Z","iopub.status.idle":"2024-07-07T11:25:24.524067Z","shell.execute_reply.started":"2024-07-07T11:25:23.495238Z","shell.execute_reply":"2024-07-07T11:25:24.523362Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4530a8eb37774578a37b636282eb19be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0ab5c44b824b2886d0b79637eb04e8"}},"metadata":{}}]},{"cell_type":"markdown","source":"Le code suivant va entra√Æner le mod√®le en utilisant la m√©thode `trainer.train()` et ensuite sauvegarder le mod√®le entra√Æn√© dans le r√©pertoire `trained-model`. En utilisant le GPU standard P100 offert par Kaggle, l'entra√Ænement devrait √™tre assez rapide.","metadata":{}},{"cell_type":"code","source":"# train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:25:24.525085Z","iopub.execute_input":"2024-07-07T11:25:24.525386Z","iopub.status.idle":"2024-07-07T11:47:22.939705Z","shell.execute_reply.started":"2024-07-07T11:25:24.525359Z","shell.execute_reply":"2024-07-07T11:47:22.938735Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 21:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.597200</td>\n      <td>0.521008</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=225, training_loss=0.6398372311062283, metrics={'train_runtime': 1317.8282, 'train_samples_per_second': 0.683, 'train_steps_per_second': 0.171, 'total_flos': 4056091312422912.0, 'train_loss': 0.6398372311062283, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Le mod√®le et le tokenizer sont sauvegard√©s sur le disque pour une utilisation ult√©rieure.","metadata":{}},{"cell_type":"code","source":"trainer.save_model()\ntokenizer.save_pretrained(\"trained_weigths\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:47:34.672223Z","iopub.execute_input":"2024-07-07T11:47:34.672607Z","iopub.status.idle":"2024-07-07T11:47:37.784898Z","shell.execute_reply.started":"2024-07-07T11:47:34.672578Z","shell.execute_reply":"2024-07-07T11:47:37.783906Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('trained_weigths/tokenizer_config.json',\n 'trained_weigths/special_tokens_map.json',\n 'trained_weigths/tokenizer.model',\n 'trained_weigths/added_tokens.json',\n 'trained_weigths/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"Ensuite, charger l'extension TensorBoard et d√©marrer TensorBoard, en pointant vers le r√©pertoire `logs/runs`, qui est cens√© contenir les journaux d'entra√Ænement et les checkpoints de votre mod√®le, vous permettra de comprendre comment le mod√®le s'ajuste pendant l'entra√Ænement.","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs/runs","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:48:58.505408Z","iopub.execute_input":"2024-07-07T11:48:58.506157Z","iopub.status.idle":"2024-07-07T11:48:58.518549Z","shell.execute_reply.started":"2024-07-07T11:48:58.506129Z","shell.execute_reply":"2024-07-07T11:48:58.517706Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 430), started 0:00:57 ago. (Use '!kill 430' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"## Sauvegarde du mod√®le üíæ\n\n√Ä ce stade, afin de d√©montrer comment r√©utiliser le mod√®le, nous le rechargeons √† partir du disque et le fusionnons avec le mod√®le LLama d'origine.\n\nEn fait, lorsque nous travaillons avec QLoRA, nous n'entra√Ænons exclusivement que des adaptateurs au lieu du mod√®le entier. Ainsi, lorsque vous sauvegardez le mod√®le pendant l'entra√Ænement, vous ne conservez que les poids des adaptateurs, pas l'ensemble du mod√®le. Si vous souhaitez sauvegarder le mod√®le complet pour une utilisation plus facile avec l'inf√©rence de g√©n√©ration de texte, vous pouvez fusionner les poids des adaptateurs avec les poids du mod√®le en utilisant la m√©thode `merge_and_unload`. Ensuite, vous pouvez sauvegarder le mod√®le en utilisant la m√©thode `save_pretrained`. Cela cr√©era un mod√®le par d√©faut pr√™t pour les t√¢ches d'inf√©rence.","metadata":{}},{"cell_type":"markdown","source":"Avant de continuer, nous commen√ßons par supprimer le mod√®le pr√©c√©dent et nettoyer la m√©moire des diff√©rents objets que nous n'utiliserons plus.","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\ndel [df, X_train, X_eval]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:48:02.298919Z","iopub.execute_input":"2024-07-07T11:48:02.299202Z","iopub.status.idle":"2024-07-07T11:48:02.304762Z","shell.execute_reply.started":"2024-07-07T11:48:02.299178Z","shell.execute_reply":"2024-07-07T11:48:02.303883Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"for _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:48:02.306040Z","iopub.execute_input":"2024-07-07T11:48:02.306299Z","iopub.status.idle":"2024-07-07T11:48:31.766887Z","shell.execute_reply.started":"2024-07-07T11:48:02.306276Z","shell.execute_reply":"2024-07-07T11:48:31.766113Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:48:31.770352Z","iopub.execute_input":"2024-07-07T11:48:31.770641Z","iopub.status.idle":"2024-07-07T11:48:32.861413Z","shell.execute_reply.started":"2024-07-07T11:48:31.770617Z","shell.execute_reply":"2024-07-07T11:48:32.860241Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Sun Jul  7 11:48:32 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   50C    P0             34W /  250W |     361MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ensuite, nous pouvons proc√©der √† la fusion des poids et utiliser le mod√®le fusionn√© √† des fins de test.","metadata":{}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nfinetuned_model = \"./trained_weigths/\"\ncompute_dtype = getattr(torch, \"float16\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n     finetuned_model,\n     torch_dtype=compute_dtype,\n     return_dict=False,\n     low_cpu_mem_usage=True,\n     device_map=device,\n)\n\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(\"./merged_model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:49:49.731858Z","iopub.execute_input":"2024-07-07T11:49:49.732516Z","iopub.status.idle":"2024-07-07T11:51:34.276172Z","shell.execute_reply.started":"2024-07-07T11:49:49.732481Z","shell.execute_reply":"2024-07-07T11:51:34.275175Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1156f0231f4bca91f3e818147deb87"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"('./merged_model/tokenizer_config.json',\n './merged_model/special_tokens_map.json',\n './merged_model/tokenizer.model',\n './merged_model/added_tokens.json',\n './merged_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test üë®üèæ‚Äçüíª\n\nLe code suivant pr√©dira d'abord les √©tiquettes de sentiment pour l'ensemble de test en utilisant la fonction `predict()`. Ensuite, il √©valuera les performances du mod√®le sur l'ensemble de test en utilisant la fonction `evaluate()`. Les r√©sultats devraient maintenant √™tre impressionnants avec une pr√©cision globale de plus de 0.8 et une pr√©cision √©lev√©e, ainsi qu'un rappel √©lev√© pour les √©tiquettes de sentiment individuelles. La pr√©diction de l'√©tiquette neutre peut encore √™tre am√©lior√©e, mais il est impressionnant de voir ce qui peut √™tre r√©alis√© avec peu de donn√©es et un peu d'ajustement fin.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test, merged_model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:51:41.857139Z","iopub.execute_input":"2024-07-07T11:51:41.857659Z","iopub.status.idle":"2024-07-07T11:56:02.159613Z","shell.execute_reply.started":"2024-07-07T11:51:41.857626Z","shell.execute_reply":"2024-07-07T11:56:02.158658Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [04:20<00:00,  3.46it/s]","output_type":"stream"},{"name":"stdout","text":"Pr√©cision: 0.858\n\nRapport de classification:\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96       300\n           1       0.78      0.81      0.79       300\n           2       0.85      0.80      0.82       300\n\n    accuracy                           0.86       900\n   macro avg       0.86      0.86      0.86       900\nweighted avg       0.86      0.86      0.86       900\n\n\nMatrice de Confusion:\n[[288  12   0]\n [ 13 244  43]\n [  2  58 240]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Le code suivant cr√©era un DataFrame Pandas appel√© `evaluation` contenant le texte, les √©tiquettes r√©elles et les √©tiquettes pr√©dites de l'ensemble de test. Cela est particuli√®rement utile pour comprendre les erreurs que le mod√®le finement r√©gl√© commet et obtenir des insights sur la fa√ßon d'am√©liorer le prompt.","metadata":{}},{"cell_type":"code","source":"evaluation = pd.DataFrame({'text': X_test[\"text\"], \n                           'y_true':y_true, \n                           'y_pred': y_pred},\n                         )\nevaluation.to_csv(\"test_predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T11:57:06.454830Z","iopub.execute_input":"2024-07-07T11:57:06.455478Z","iopub.status.idle":"2024-07-07T11:57:06.490809Z","shell.execute_reply.started":"2024-07-07T11:57:06.455447Z","shell.execute_reply":"2024-07-07T11:57:06.490093Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
