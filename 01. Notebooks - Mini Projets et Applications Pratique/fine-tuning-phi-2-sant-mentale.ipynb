{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine-tuning du mod√®le Microsoft Phi-3 pour la fourniture de conseils en sant√© mentale**\n\n\n#### Probl√©matique\nL'am√©lioration des mod√®les de langage pour fournir des conseils en sant√© mentale est d'une importance capitale. En effet, de nombreuses personnes recherchent des conseils et du soutien en ligne pour faire face √† des probl√®mes de sant√© mentale. Un mod√®le capable de g√©n√©rer des r√©ponses pr√©cises, empathiques et utiles peut non seulement am√©liorer l'acc√®s aux ressources de sant√© mentale, mais aussi all√©ger la charge des professionnels en offrant des r√©ponses initiales et en orientant les utilisateurs vers des solutions adapt√©es. Le fine-tuning du mod√®le Microsoft Phi-3 sur un dataset sp√©cialis√© permettrait de renforcer ses capacit√©s √† fournir des r√©ponses pertinentes et bienveillantes dans ce domaine sensible.\n\n#### Dataset\nLe dataset \"Amod/mental_health_counseling_conversations\" est une collection de questions et de r√©ponses issues de deux plateformes en ligne de counseling et de th√©rapie. Les questions couvrent une large gamme de sujets li√©s √† la sant√© mentale, et les r√©ponses sont fournies par des psychologues qualifi√©s.\n\n- **Context** : Il s'agit de la question pos√©e par un utilisateur, repr√©sentant les pr√©occupations ou les probl√®mes sp√©cifiques li√©s √† la sant√© mentale.\n- **Response** : Il s'agit de la r√©ponse fournie par un psychologue, contenant des conseils et des orientations pour aider l'utilisateur.\n\n\nCe dataset est id√©al pour le fine-tuning du mod√®le Microsoft Phi-3 dans le but d'am√©liorer ses capacit√©s √† g√©n√©rer des conseils en sant√© mentale, en combinant pr√©cision, pertinence et empathie dans ses r√©ponses.","metadata":{}},{"cell_type":"markdown","source":"# Biblioth√®ques & Installations üóÇ¬∂\n\nImports importants expliqu√©s et exemple d'utilisation basique :\n\n#### datasets\n- **But** : biblioth√®que pour charger et traiter facilement les ensembles de donn√©es de HF.\n- **Utilisation Basique** :\n  ```python\n  from datasets import load_dataset, load_from_disk\n  dataset = load_dataset('path/to/dataset', split='train')\n  ```\n\n#### peft\n- **But** : fournit des utilitaires pour un ajustement fin efficace des param√®tres.\n- **Utilisation Basique** :\n  ```python\n  from peft import LoraConfig, prepare_model_for_kbit_training\n  lora_config = LoraConfig()\n  ```\n\n#### transformers\n- **But** : fournit des classes et des fonctions pour les mod√®les transformers.\n- **Utilisation Basique** :\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n  model = AutoModelForCausalLM.from_pretrained('model_name')\n  tokenizer = AutoTokenizer.from_pretrained('model_name')\n  ```\n\n#### trl\n- **But** : fournit des utilitaires pour entra√Æner des mod√®les transformers avec l'apprentissage par renforcement (mais pas que).\n- **Utilisation Basique** :\n  ```python\n  from trl import SFTTrainer\n  trainer = SFTTrainer(model, tokenizer)\n  ```","metadata":{}},{"cell_type":"code","source":"!pip install -q torch peft bitsandbytes scipy trl transformers accelerate einops tqdm huggingface_hub --use-deprecated=legacy-resolver","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-07T20:17:43.584028Z","iopub.execute_input":"2024-07-07T20:17:43.584586Z","iopub.status.idle":"2024-07-07T20:18:04.161001Z","shell.execute_reply.started":"2024-07-07T20:17:43.584557Z","shell.execute_reply":"2024-07-07T20:18:04.160081Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you'll have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset, load_from_disk\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments \nfrom trl import SFTTrainer\nfrom huggingface_hub import notebook_login\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:18:04.163307Z","iopub.execute_input":"2024-07-07T20:18:04.163623Z","iopub.status.idle":"2024-07-07T20:18:23.354921Z","shell.execute_reply.started":"2024-07-07T20:18:04.163590Z","shell.execute_reply":"2024-07-07T20:18:23.354118Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-07 20:18:14.044959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-07 20:18:14.045066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-07 20:18:14.171869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Version pytorch --> {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:19.982712Z","iopub.execute_input":"2024-07-07T20:20:19.983509Z","iopub.status.idle":"2024-07-07T20:20:19.988128Z","shell.execute_reply.started":"2024-07-07T20:20:19.983477Z","shell.execute_reply":"2024-07-07T20:20:19.987158Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Version pytorch --> 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"device --> {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:21.888795Z","iopub.execute_input":"2024-07-07T20:20:21.889125Z","iopub.status.idle":"2024-07-07T20:20:21.894465Z","shell.execute_reply.started":"2024-07-07T20:20:21.889098Z","shell.execute_reply":"2024-07-07T20:20:21.893574Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"device --> cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"### pour se connecter √† Hugging Face\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:23.401971Z","iopub.execute_input":"2024-07-07T20:20:23.402460Z","iopub.status.idle":"2024-07-07T20:20:23.424404Z","shell.execute_reply.started":"2024-07-07T20:20:23.402430Z","shell.execute_reply":"2024-07-07T20:20:23.423550Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b9fb71033f414ba98b1efc10c3241b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Pr√©paration des donn√©es üõ¢","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:33.889100Z","iopub.execute_input":"2024-07-07T20:20:33.889483Z","iopub.status.idle":"2024-07-07T20:20:36.483122Z","shell.execute_reply.started":"2024-07-07T20:20:33.889454Z","shell.execute_reply":"2024-07-07T20:20:36.482233Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b3870f04b748dc8b0aa836555a8c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5e2a80333264a0587960b4a36d5d9e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e92e4cf10b47cf80d0e1048069c373"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Context', 'Response'],\n    num_rows: 3512\n})"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.DataFrame(dataset)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:36.484643Z","iopub.execute_input":"2024-07-07T20:20:36.484930Z","iopub.status.idle":"2024-07-07T20:20:36.660990Z","shell.execute_reply.started":"2024-07-07T20:20:36.484905Z","shell.execute_reply":"2024-07-07T20:20:36.659618Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n3  I'm going through some things with my feelings...   \n4  I'm going through some things with my feelings...   \n\n                                            Response  \n0  If everyone thinks you're worthless, then mayb...  \n1  Hello, and thank you for your question and see...  \n2  First thing I'd suggest is getting the sleep y...  \n3  Therapy is essential for those that are feelin...  \n4  I first want to let you know that you are not ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>If everyone thinks you're worthless, then mayb...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Hello, and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>First thing I'd suggest is getting the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Therapy is essential for those that are feelin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>I first want to let you know that you are not ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:36.664411Z","iopub.execute_input":"2024-07-07T20:20:36.664788Z","iopub.status.idle":"2024-07-07T20:20:36.685160Z","shell.execute_reply.started":"2024-07-07T20:20:36.664747Z","shell.execute_reply":"2024-07-07T20:20:36.684235Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3512 entries, 0 to 3511\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Context   3512 non-null   object\n 1   Response  3512 non-null   object\ndtypes: object(2)\nmemory usage: 55.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"def format_row(row):\n    question = row[\"Context\"]\n    response = row[\"Response\"]\n    formatted_string = f\"[INST] {question} [/INST] {response}\"\n    return formatted_string\n\ndf[\"Text\"] = df.apply(format_row, axis=1)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:36.938929Z","iopub.execute_input":"2024-07-07T20:20:36.939947Z","iopub.status.idle":"2024-07-07T20:20:37.003826Z","shell.execute_reply.started":"2024-07-07T20:20:36.939896Z","shell.execute_reply":"2024-07-07T20:20:37.002919Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n\n                                            Response  \\\n0  If everyone thinks you're worthless, then mayb...   \n1  Hello, and thank you for your question and see...   \n2  First thing I'd suggest is getting the sleep y...   \n\n                                                Text  \n0  [INST] I'm going through some things with my f...  \n1  [INST] I'm going through some things with my f...  \n2  [INST] I'm going through some things with my f...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>If everyone thinks you're worthless, then mayb...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Hello, and thank you for your question and see...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>First thing I'd suggest is getting the sleep y...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df = df[[\"Text\"]]\ntrain, test = train_test_split(new_df, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:37.718622Z","iopub.execute_input":"2024-07-07T20:20:37.719399Z","iopub.status.idle":"2024-07-07T20:20:37.729491Z","shell.execute_reply.started":"2024-07-07T20:20:37.719363Z","shell.execute_reply":"2024-07-07T20:20:37.728472Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train_data.csv\", index=False)\ntest.to_csv(\"test_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:38.551791Z","iopub.execute_input":"2024-07-07T20:20:38.552444Z","iopub.status.idle":"2024-07-07T20:20:38.757989Z","shell.execute_reply.started":"2024-07-07T20:20:38.552410Z","shell.execute_reply":"2024-07-07T20:20:38.756979Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_dataset(\"csv\", data_files=\"train_data.csv\", split=\"train\")\ntest_dataset  = load_dataset(\"csv\", data_files=\"test_data.csv\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:39.308014Z","iopub.execute_input":"2024-07-07T20:20:39.308381Z","iopub.status.idle":"2024-07-07T20:20:39.882006Z","shell.execute_reply.started":"2024-07-07T20:20:39.308351Z","shell.execute_reply":"2024-07-07T20:20:39.881113Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01543c53936a44f5af7c64a04cc5524d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23fc4fd8fa67410dbf8a7bd3b6aba272"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:40.081009Z","iopub.execute_input":"2024-07-07T20:20:40.081373Z","iopub.status.idle":"2024-07-07T20:20:40.087427Z","shell.execute_reply.started":"2024-07-07T20:20:40.081344Z","shell.execute_reply":"2024-07-07T20:20:40.086526Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Text'],\n    num_rows: 2809\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning ü¶æ","metadata":{}},{"cell_type":"code","source":"base_model = \"microsoft/phi-2\"\nnew_model  = \"phi2-ft-mental-health\"","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:43.958342Z","iopub.execute_input":"2024-07-07T20:20:43.959015Z","iopub.status.idle":"2024-07-07T20:20:43.963008Z","shell.execute_reply.started":"2024-07-07T20:20:43.958981Z","shell.execute_reply":"2024-07-07T20:20:43.962052Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenizer\n\n1. **Chargement du Tokenizer** (`AutoTokenizer.from_pretrained`) : charge un tokenizer pr√©-entra√Æn√© pour le mod√®le sp√©cifi√©.\n     - **Param√®tres** :\n       - `base_model` : identifiant du mod√®le pr√©-entra√Æn√© (par exemple, un nom de mod√®le ou un chemin).\n       - `use_fast=True` : sp√©cifie que la version rapide du tokenizer doit √™tre utilis√©e. Les tokenizers rapides sont g√©n√©ralement plus efficaces et plus rapides.  \n\n2. **D√©finition du token de padding** (`tokenizer.pad_token = tokenizer.eos_token`) : d√©finit le jeton de remplissage du tokenizer pour qu'il soit le m√™me que le jeton de fin de s√©quence (EOS). Dans certains mod√®les et configurations d'entra√Ænement, il est utile d'utiliser le jeton EOS √† des fins de remplissage pour garantir la coh√©rence du processus de tokenisation.\n\n3. **Sp√©cification du C√¥t√© de Remplissage** (`tokenizer.padding_side = \"right\"`) : sp√©cifie de quel c√¥t√© de la s√©quence les jetons de remplissage doivent √™tre ajout√©s.\n   - **Options** :\n     - `\"right\"` : Les jetons de remplissage sont ajout√©s du c√¥t√© droit de la s√©quence.\n     - `\"left\"` : Les jetons de remplissage sont ajout√©s du c√¥t√© gauche de la s√©quence.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:20:47.286691Z","iopub.execute_input":"2024-07-07T20:20:47.287055Z","iopub.status.idle":"2024-07-07T20:20:49.161808Z","shell.execute_reply.started":"2024-07-07T20:20:47.287024Z","shell.execute_reply":"2024-07-07T20:20:49.160998Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa21556182e4f42ad3c839cb90a5e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3316a52d05643c6a588387d1cd27b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb6983ab95f46d08fb3d43515a188ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166d1c65ffed4b78a68b682644cd07ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4eff6a31d5c4362aafa9217dc68e806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0416b43f014747cfaab1330f87576ddf"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### BitsAndBytesConfig\n\n1. **Classe** : `BitsAndBytesConfig`\n2. **Param√®tres** :\n   - `load_in_4bit=True` : activer ou non le chargement du mod√®le en pr√©cision 4 bits (√©conomise la m√©moire et acc√©l√®re les calculs).\n   - `bnb_4bit_quant_type=\"nf4\"` : Sp√©cifie le type de quantification pour la pr√©cision 4 bits.\n     - **Options** : \n       - `\"nf4\"` : Quantification Non-Flottante 4 bits, plus efficace en termes de m√©moire.\n       - `\"fp4\"` : Quantification en Virgule Flottante 4 bits, offrant une plus grande pr√©cision.\n   - `bnb_4bit_compute_dtype=torch.float16` : d√©finit le type de donn√©es pour les calculs avec une pr√©cision de 4 bits.\n     - **Options** : \n       - `torch.float16` : Utilise des flottants 16 bits pour des calculs plus rapides avec moins de m√©moire utilis√©e.\n       - `torch.float32` : Utilise des flottants 32 bits pour une plus grande pr√©cision mais plus de m√©moire utilis√©e.\n   - `bnb_4bit_use_double_quant=False` : d√©termine s'il faut utiliser la double quantification, qui applique la quantification deux fois pour une meilleure pr√©cision (`True` : applique la double quantification pour une meilleure pr√©cision, `False` : ne pas appliquer).\n\n#### LoraConfig\n\n1. **Classe** : `LoraConfig`\n2. **Param√®tres** :\n   - `r=32` : rang de la matrice de faible rang dans LoRA, contr√¥lant la capacit√© de l'adaptation.\n     - **Options** : Valeurs enti√®res (par exemple, 4, 8, 16, 32). Des valeurs plus √©lev√©es augmentent la capacit√© mais n√©cessitent plus de calculs.\n   - `lora_alpha=64` : un facteur de mise √† l'√©chelle pour les mises √† jour de faible rang, affectant le taux d'apprentissage de l'adaptation.\n     - **Options** : Valeurs enti√®res (par exemple, 16, 32, 64, 128). Des valeurs plus √©lev√©es peuvent entra√Æner des mises √† jour plus importantes.\n   - `lora_dropout=0.05` : le taux de dropout appliqu√© aux mises √† jour de faible rang pour √©viter le surajustement.\n     - **Options** : Valeurs flottantes entre 0 et 1 (par exemple, 0.1, 0.2, 0.5). Des valeurs plus √©lev√©es augmentent la r√©gularisation.\n   - `bias_type=\"none\"` : Sp√©cifie comment g√©rer les termes de biais dans les couches LoRA.\n     - **Options** : \n       - `\"none\"` : aucun terme de biais n'est adapt√©.\n       - `\"all\"` : tous les termes de biais sont adapt√©s.\n       - `\"some\"` : seuls certains termes de biais sont adapt√©s.\n   - `task_type=\"CAUSAL_LM\"` : type de t√¢che pour laquelle LoRA est utilis√©.\n     - **Options** : \n       - `\"CAUSAL_LM\"` : Mod√©lisation de Langage Causal, pour des t√¢ches auto-r√©gressives.\n       - `\"SEQ2SEQ_LM\"` : Mod√©lisation de Langage S√©quence-√†-S√©quence, pour des t√¢ches de traduction ou de r√©sum√©.\n   - `target_modules=[\"Wqkv\", \"fc1\", \"fc2\"]` : couches du mod√®le o√π LoRA sera appliqu√©.\n     - **Options** : Liste de noms de couches (par exemple, `[\"Wqkv\"]`, `[\"fc1\"]`, `[\"fc2\"]`). Sp√©cifique √† l'architecture du mod√®le √† ajuster.","metadata":{}},{"cell_type":"code","source":"bnb_configs = BitsAndBytesConfig(   load_in_4bit=True,\n                                    bnb_4bit_quant_type=\"nf4\",\n                                    bnb_4bit_compute_dtype=torch.float16,\n                                    bnb_4bit_use_double_quant=True\n                                )\n\npeft_configs = LoraConfig(  r=8,\n                            lora_alpha=32,\n                            lora_dropout=0.1,\n                            bias=\"none\",\n                            task_type=\"CAUSAL_LM\",\n                            target_modules=\"all-linear\"\n                         )","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:28:31.942860Z","iopub.execute_input":"2024-07-07T20:28:31.943602Z","iopub.status.idle":"2024-07-07T20:28:31.950408Z","shell.execute_reply.started":"2024-07-07T20:28:31.943564Z","shell.execute_reply":"2024-07-07T20:28:31.949532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Initialisation et configurations du mod√®le\n\n1. **Initialisation du Mod√®le** (`AutoModelForCausalLM.from_pretrained`) : charge un mod√®le de langage causal pr√©-entra√Æn√©.\n   - **Param√®tres** :\n     - `base_model` : identifiant du mod√®le pr√©-entra√Æn√© (par exemple, nom ou chemin du mod√®le).\n     - `flash_attn=True` : active le m√©canisme d'attention Flash, qui optimise les m√©canismes d'attention pour am√©liorer les performances.\n     - `flash_rotary=True` : active le m√©canisme Rotary Flash, qui am√©liore les embeddings rotatifs pour un meilleur apprentissage de la repr√©sentation.\n     - `fused_dense=True` : utilise des op√©rations denses fusionn√©es, combinant plusieurs op√©rations en un seul noyau pour plus d'efficacit√©.\n     - `low_cpu_mem_usage=True` : optimise l'utilisation de la m√©moire CPU, r√©duisant l'empreinte m√©moire lors de l'ex√©cution du mod√®le.\n     - `device_map={\"\": 0}` : mappe les appareils pour les composants du mod√®le.\n     - `revision=\"refs/pr/23\"` : sp√©cifie une r√©vision sp√©cifique du mod√®le √† charger.\n\n2. **Configurations du Mod√®le** :\n   - `model.config.use_cache = False` : d√©sactive la mise en cache des calculs internes dans le mod√®le.\n   - `model.config.pretraining_tp = 1` : d√©finit un param√®tre sp√©cifique de la t√¢che de pr√©-entra√Ænement √† 1.\n\n3. **Pr√©paration du Mod√®le pour l'entra√Ænement en k-bit** (`prepare_model_for_kbit_training`) : pr√©pare le mod√®le pour l'entra√Ænement avec quantification en k-bit.\n   - **Param√®tres** :\n     - `model` : L'instance du mod√®le √† pr√©parer pour l'entra√Ænement.\n     - `use_gradient_checkpointing=True` : Active le gradient checkpointing pour l'efficacit√© de la m√©moire pendant l'entra√Ænement.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n                                base_model,\n                                quantization_config=bnb_configs,\n                                torch_dtype=torch.float16,\n                                trust_remote_code=True,\n                                flash_attn=True,\n                                flash_rotary=True,\n                                fused_dense=True,\n                                low_cpu_mem_usage=True,\n                                device_map=device,\n                                revision=\"refs/pr/23\"\n                                )\n\n#model.to(device)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:28:32.180988Z","iopub.execute_input":"2024-07-07T20:28:32.181694Z","iopub.status.idle":"2024-07-07T20:28:35.980124Z","shell.execute_reply.started":"2024-07-07T20:28:32.181661Z","shell.execute_reply":"2024-07-07T20:28:35.979234Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb90d8a79a1481596dccf144110da0a"}},"metadata":{}},{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Entra√Ænement du mod√®le ü§ñ\n\n**Param√®tres des arguments d'entra√Ænement** (`TrainingArguments`) :\n - `num_train_epochs=1` : nombre de fois que le mod√®le sera entra√Æn√© sur l'ensemble du jeu de donn√©es.\n - `per_device_train_batch_size=2` : nombre d'√©chantillons d'entra√Ænement trait√©s simultan√©ment sur chaque appareil (GPU ou CPU).\n - `gradient_accumulation_steps=32` : nombre de lots pour accumuler les gradients avant d'effectuer une passe arri√®re.\n   - **Objectif** : aide √† l'entra√Ænement avec des tailles de lot effectives plus grandes que la m√©moire ne le permet, utile lorsque la m√©moire GPU est limit√©e.\n   - **Options** : valeurs enti√®res (par exemple, 1, 2, 4, 8, etc.).\n - `evaluation_strategy=\"steps\"` : d√©termine quand effectuer une √©valuation pendant l'entra√Ænement.\n   - **Objectif** : sp√©cifie la strat√©gie pour √©valuer le mod√®le pendant l'entra√Ænement, bas√©e sur les √©tapes, les √©poques ou aucune √©valuation.\n   - **Options** : `\"no\"` (pas d'√©valuation), `\"steps\"` (√©valuation tous les `eval_steps`), `\"epoch\"` (√©valuation √† la fin de chaque √©poque).\n - `eval_steps=1500` : intervalle en √©tapes pour l'√©valuation si `evaluation_strategy=\"steps\"`.\n - `logging_steps=1500` : intervalle en √©tapes pour la journalisation des m√©triques d'entra√Ænement dans la console ou les fichiers.\n - `optim=\"paged_adamw_8bit\"` : type d'optimiseur utilis√© pour l'entra√Ænement, ici utilisant **paged AdamW avec une pr√©cision de 8 bits**.\n   - **Options** : d√©pend de l'impl√©mentation sp√©cifique et des optimiseurs disponibles.\n - `learning_rate=2e-4` : taux d'apprentissage initial pour l'optimiseur.\n   - **Objectif** : contr√¥le la taille des pas pendant la descente de gradient ou l'optimisation.\n   - **Options** : valeurs flottantes (par exemple, 0.001, 0.0001, etc.).\n - `lr_scheduler_type=\"cosine\"` : type de planificateur de taux d'apprentissage appliqu√© pendant l'entra√Ænement.\n   - **Objectif** : ajuste le taux d'apprentissage pendant l'entra√Ænement pour optimiser la convergence du mod√®le.\n   - **Options** : `\"linear\"`, `\"cosine\"`, `\"step\"`, `\"polynomial\"`, etc., selon l'impl√©mentation du planificateur.\n - `save_steps=1500` : intervalle en √©tapes pour enregistrer les points de contr√¥le du mod√®le.\n - `warmup_ratio=0.05` : ratio du nombre total d'√©tapes d'entra√Ænement pour lesquelles le taux d'apprentissage sera augment√© progressivement.\n   - **Objectif** : emp√™che le mod√®le de diverger pendant les premi√®res √©tapes de l'entra√Ænement en augmentant lentement le taux d'apprentissage.\n   - **Options** : valeurs flottantes entre 0 et 1 (par exemple, 0.1, 0.05, etc.).\n - `weight_decay=0.01` : force de la r√©gularisation de la d√©croissance des poids appliqu√©e aux param√®tres du mod√®le pendant l'optimisation.\n   - **Objectif** : aide √† pr√©venir le surapprentissage en p√©nalisant les poids √©lev√©s.\n   - **Options** : valeurs flottantes (par exemple, 0.001, 0.01, etc.).\n - `max_steps=-1` : nombre maximum d'√©tapes d'entra√Ænement ; `-1` indique un nombre illimit√© d'√©tapes.\n   - **Objectif** : limite le nombre d'it√©rations que le mod√®le subira pendant l'entra√Ænement.\n   - **Options** : valeurs enti√®res ou `-1` pour un nombre illimit√© d'√©tapes d'entra√Ænement.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir=\"trained_weigths\",                          ### r√©pertoire pour sauvegarder et identifiant du r√©f√©rentiel\n                                num_train_epochs=1,                       ### nombre d'√©poques d'entra√Ænement\n                                per_device_train_batch_size=1,            ### taille du batch par p√©riph√©rique pendant l'entra√Ænement\n                                gradient_accumulation_steps=4,            ### nombre d'√©tapes avant d'effectuer une passe de r√©tropropagation/mise √† jour\n                                gradient_checkpointing=True,              ### utilise le point de contr√¥le de gradient pour √©conomiser de la m√©moire\n                                optim=\"paged_adamw_32bit\",\n                                save_steps=0,\n                                logging_steps=25,                         ### enregistre chaque 25 √©tapes\n                                learning_rate=2e-4,                       ### taux d'apprentissage, bas√© sur le papier QLoRA\n                                weight_decay=0.001,\n                                fp16=True,\n                                bf16=False,\n                                max_grad_norm=0.3,                        ### norme maximale du gradient bas√©e sur le papier QLoRA\n                                max_steps=-1,\n                                warmup_ratio=0.03,                        ### ratio de pr√©chauffage bas√© sur le papier QLoRA\n                                group_by_length=True,\n                                lr_scheduler_type=\"cosine\",               ### utilise le programmeur de taux d'apprentissage cosinus\n                                evaluation_strategy=\"steps\"               ### sauvegarde le checkpoint √† chaque √©poque\n                                 ) \n\n\ntrainer = SFTTrainer(   model=model,\n                        train_dataset=train_dataset,\n                        eval_dataset=test_dataset,\n                        peft_config=peft_configs,\n                        dataset_text_field=\"Text\",\n                        max_seq_length=690,\n                        tokenizer=tokenizer,\n                        args=training_args\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:28:35.981644Z","iopub.execute_input":"2024-07-07T20:28:35.981931Z","iopub.status.idle":"2024-07-07T20:28:38.715996Z","shell.execute_reply.started":"2024-07-07T20:28:35.981906Z","shell.execute_reply":"2024-07-07T20:28:38.715239Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\nYou are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2809 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c60404f1f849cabb43452c62529a26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/703 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5802d16e4d264053adfcfa9a0fdfb584"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:28:38.717111Z","iopub.execute_input":"2024-07-07T20:28:38.717472Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240707_202903-6tqw60vz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/anyantudre-ENSA%20Safi/huggingface/runs/6tqw60vz' target=\"_blank\">trained_weigths</a></strong> to <a href='https://wandb.ai/anyantudre-ENSA%20Safi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/anyantudre-ENSA%20Safi/huggingface' target=\"_blank\">https://wandb.ai/anyantudre-ENSA%20Safi/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/anyantudre-ENSA%20Safi/huggingface/runs/6tqw60vz' target=\"_blank\">https://wandb.ai/anyantudre-ENSA%20Safi/huggingface/runs/6tqw60vz</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  8/702 00:35 < 1:08:50, 0.17 it/s, Epoch 0.01/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Push to hub","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./trained_model\")\nmodel.push_to_hub(\"anyantudre/phi-2-ft-mental_health\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}