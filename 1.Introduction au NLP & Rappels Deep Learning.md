# Introduction au NLP & Rappels Deep Learning.md


### Plan
I. Le Traitement Automatique du Language Naturel(TALN ou NLP)
    1. DÃ©finition
    2. Les Applications
    3. BibliothÃ¨ques
    4. Le pipeline classique en NLP
   

II. Bref rappel sur l'histoire des RÃ©seaux de Neurones
    1. Le Perceptron
    2. Le Perceptron Multi Couches (MLP ou PMC)
    3. Couches denses : Transformation de la reprÃ©sentation des donnÃ©es
  
III. 1er Mini Projet: Analyse de sentiments




# I. Le Traitement Automatique du Language Naturel(TALN ou NLP)
## 1. DÃ©finition
ðŸ—£ï¸ ððšð­ð®ð«ðšð¥ ð‹ðšð§ð ð®ðšð ðž ðð«ð¨ðœðžð¬ð¬ð¢ð§ð  (or NLP for short) is an engineering discipline that enables computers to understand, generate, and manipulate human language in the way that it is written, spoken, and organized.
NLP can be divided into two overlapping subfields: 
- Natural Language Understanding (NLU), which focuses on semantic analysis (meaning of a text).
- Natural Language Generation (NLG), which focuses on text generation.


## 2. Les Applications
NLP is used for a wide variety of language-related tasks:
- ð“ðžð±ð­ ðœð¥ðšð¬ð¬ð¢ðŸð¢ðœðšð­ð¢ð¨ð§: the process of automatically categorizing or labeling text data into predefined categories or classes based on its content and context. Sub-tasks include:
     o  Sentiment Analysis: identifying emotional intent in text, which aids in customer review classification for example.
     o  Toxicty detection: detecting hate speech or defamation in online content.
     o  Spam Detection: flagging unsolicited emails for removal.

- ðŒðšðœð¡ð¢ð§ðž ð­ð«ðšð§ð¬ð¥ðšð­ð¢ð¨ð§: that allows to automate translation between different languages. Google Translate and DeepL are probably the most famous mainstream applications.

- ð“ðžð±ð­ ð ðžð§ðžð«ðšð­ð¢ð¨ð§: probably the most popular application of NLP right now with Generative AI. It aims to produce text similar to human-written text, including:
     o  Conversation agents and virtual assistants (Chatbots) for questions answering,
     o  Autocompletion, predicting the next word,
     o  Masked words filling etc.

- ðð„ð‘ (Named Entity Recognition): identifying and extracting useful entities such as personal names, organizations and locations in text.

- ð“ð¨ð©ð¢ðœ ð¦ð¨ððžð¥ð¢ð§ð : an unsupervised text mining task that discovers abstract topics within a corpus of documents.

- ð’ð®ð¦ð¦ðšð«ð¢ð³ðšð­ð¢ð¨ð§: task of shortening or condensing text to highlight the most relevant information.

Moreover, NLP isnâ€™t limited to written text though; it also tackles complex challenges in ð¬ð©ðžðžðœð¡ ð«ðžðœð¨ð ð§ð¢ð­ð¢ð¨ð§ and ðœð¨ð¦ð©ð®ð­ðžð« ð¯ð¢ð¬ð¢ð¨ð§, such as generating transcripts of audio samples or descriptions of images.

    
## 3. BibliothÃ¨ques

In the field of IT in general, the choice of working tools is often complicated by the diversity of available tools ðŸ› ðŸ§°.

The same is true of Natural Language Processing (NLP).



NLTK and spaCy are probably the two most popular and widely used tools/libraries for NLP (in Python of course), although there are many other interesting tools.



What are their specific features?

- ð¬ð©ðšð‚ð²: at the top of my list, this is my favorite NLP library to date. spaCy is versatile, open-source, object-oriented and comes with pre-trained statistical models (like the famous BERT) and word vectors. It supports over 66 languages and can be used to build production-ready systems for NER, sentence segmentation, text classification, lemmatization etc. 

Recently, spacy-llm was released to enable the integration of Large Language Models (LLMs) into spaCy.

 

- ðð‹ð“ðŠ (Natural Language Toolkit): one of the oldest NLP libraries, written in Python and used in both academia and industry. It provides easy-to-use interfaces to corpora and lexical resources such as WordNet. 

It also provides a suite of text processing libraries for classification, tagging, tokenization, stemming, parsing and semantic reasoning. 

Apparently, NLTK, which by the way is string-based, is preferred by many for its comprehensive documentation and educational resources. 

 



Of course, there are also popular Deep Learning libraries, such as ð“ðžð§ð¬ð¨ð«ð…ð¥ð¨ð°, ðŠðžð«ðšð¬ ðšð§ð ðð²ð“ð¨ð«ðœð¡, that facilitate the creation of NLP models, but I won't dwell on them in this post.

 

In addition, other notable libraries purely for NLP are :



- ð…ðšð¬ð­ð“ðžð±ð­: developed by Facebook AI, it enables users to learn text representations and classifiers with fast training speed and scalability using supervised and unsupervised learning algorithms.



- ð†ðžð§ð¬ð¢ð¦: useful for unsupervised learning tasks such as document similarity analysis, topic modeling and word embedding techniques.

 

Last but not least on the list is ð‡ð®ð ð ð¢ð§ð  ð…ðšðœðžðŸ¤— and its ð“ð«ðšð§ð¬ðŸð¨ð«ð¦ðžð«ð¬ library. It provides thousands of state-of-the-art pre-trained models for a variety of NLP tasks, including BERT, GPT, RoBERTa, and many others. 

The platform makes it easy to customize and train the models.

 

So much for NLP tools!



Of course, every NLP practitioner has his/her own preferred tool, but the choice should be based on the objectives of the task in hand, not forgetting potential constraints.

 







#nlp #ai #llm #deeplearning #nltk #spacy #neuralnetworks #machinelearning #datascience
    
## 4. Le pipeline classique en NLP






## II. Bref rappel sur l'histoire des RÃ©seaux de Neurones

### Simple Dense Layer

Imagine you're an high school teacher and after end-of-year evaluations you stored the grades of your students in a table like the following one:

![Image 2](visuals/1.Mathematics-&-Gradients-everywhere/2.png)

Each row is for students and each columns for a specific course taken.

About variables in this table:

|French         | English             |
| ------------- | ------------------- |
| Mathematiques | Mathematics         |
| Physique      | Physics             |
| Informatique  | Computer Science    |
| LV1           | Modern Language 1  |
| LV2           | Modern Language 2  |
| Biologie      | Biology             |
| Chimie        | Chemistry           |


Well, so basically the problematic is 
> **Question 1: How do you highlight features or let's say extract meaningful information from this table(data)?**

- An intuive solution that schools and teachers often use is **calculating the mean of all the grades** for each student. In other words, we sum up all the grades for each student and then divide by the total number of subjects (7 here). This operation simplifies the information since we have now at hand only one column showing ...
In this case we'll got something like that:
![Image 3](visuals/1.Mathematics-&-Gradients-everywhere/3.png)

- A more general solution to the previous one is **calculating a weighted sum of the grades** in order to extract specific features/tendences for each student.  
The **general formula** is the following one, where the w_j represent the weights and the x_i_j the j_th grade of the i_th student :    
<center><img src="visuals/1.Mathematics-&-Gradients-everywhere/3.1.png" alt="Image 3.1" height="200" width="400"></center> 


    - For example, if we want to know which students are very **strong only in Modern Languages**, we can take the weight vector as follows: `w = (0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0)`.  
It simply **calculates the average of the marks obtained in the two language subjects**!  
![Image 4](visuals/1.Mathematics-&-Gradients-everywhere/4.png)

    - A second illustration comes when we want to **compare students who are excellent in mathematics and those who are excellent in physics*. Taking a weight vector `w = (1.0, 0.0, âˆ’ 1.0, 0.0, 0.0, 0.0, 0.0)`, we simply calculate **the difference between the two scores**: 
![Image 5](visuals/1.Mathematics-&-Gradients-everywhere/5.png)

    - Now let's say we want to know how each student performs in **scientific subjects**. Like in the first example we can simply compute **a weighted sum of the scientific subjects** with a weight vector given as `w = (0.2, 0.2, 0.2, 0.0, 0.0, 0.2, 0.2)`.
**The average performance in scientific subjects.**  
![Image 6](visuals/1.Mathematics-&-Gradients-everywhere/6.png)

Well you noticed that it's interesting to see in a more informative way how our students performs generally in all the subjects, and particularly in Modern Languages and in Scientific subjects... That's great, but too easy. Yes indeed, we need some more and specific information.


> **Question 2: How can we keep only the information that interests us?**


### Dense layer with bias, followed by ReLU (Rectified Linear Unit)

If the previous question is not yet clear to you, imagine from the third(last) illustration we only want to know exactly who are the students that are above the average to pass (10 in our case) in science subjects and discard the remaining ones (others that are below the passing average).  
A simple solution would be to subtract 10 from the weighted sum and keep only those students whose results are positive (above zero).

Here is an illustration:
![Image 7](visuals/1.Mathematics-&-Gradients-everywhere/7.png)

So basically, what is done here is substracting 10 to the result of the weighted sum of scientific subjects and using the ReLU function we test if the global result is greater than zero (0) then we keep it as it is but if it's negative we replace it by zero (0).  

In Deep Learning, b (value 10 here) is called **the bias term** and the function **y_i = max(0, s_i)** is the **Rectified Linear Unit called ReLU** for short.  
**Simply put, ReLU allows you to set unnecessary information to zero** (of course depending on the weightings chosen).

Let's apply same principles to Languages subjects and see which students performs well on both scientific and Languages:

![Image 8](visuals/1.Mathematics-&-Gradients-everywhere/8.png)  
You noticed we passed **from a 7 columns raw table to a 2 columns meaningful table** showing us exactly what we wanted to know i.e., the best students in Sciences and Languages. We can even go futher by repeating this process again and again with different weights and different subjects.
I think at this stage, you got the point: **this process is exactly what Dense layers in Neural Networks do.**

So to sum up, what Dense layers i.e., **several weighted sums with bias, followed by ReLU** do is:
- **Data dimension reduction** 
- **Feature extraction**
- **Information selection**


Isn't it cool? But since I'm a programmer, I'll find it more cool if I can program it :)

> **Question 3: How do we code this in a language dedicated to Neural Networks?**


### Practice with code

First let's start by importing the necessary librairies. 
Note that we're going to use PyTorch for all the Neural Networks implementations in these series.

```python
import numpy as np                # linear algebra
import pandas as pd               # data processing
import matplotlib.pyplot as plt   # visualisations

import torch                      # PyTorch
```

Now let's import the data itself using Pandas. Note that I'm working in a Kaggle environment:

```python
data = pd.read_csv("/kaggle/input/notes-table/table notes.png.csv")
```
The DataFrame looks like this:
![Image 8.1](visuals/1.Mathematics-&-Gradients-everywhere/8.1.png)



```python
data.pop("Eleve")
subjects = list(data.keys())

notes = data.to_numpy()
print(notes)
```

```python
notes_pt = torch.tensor(notes, dtype=torch.float64)

# define activation function
activation_fct_relu = torch.nn.ReLU()

# define dense layer and it parameters(7 input features; 2 output features)
linear_filter = torch.nn.Linear(7, 2, bias=True)
```

```python
linear_filter.weight.data = torch.tensor([[0.2, 0.2, 0.2, 0.0, 0.0, 0.2, 0.2],
                                         [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0]], 
                                        dtype=torch.float64)
linear_filter.bias.data   = torch.tensor([-10, -10], dtype=torch.float64)
```

```python
filtered_data = activation_fct_relu(linear_filter(notes_pt[:,:]))
print(filtered_data)
```


## 2. Convolution filters

Another fundamental building blocks in Neural Networks that allows to recognize patterns within data (typically images but not only) are **Convolutional layers**.


### Convolution 

Convolution is simply a mathematical operation used to process images or other multidimensional data. It involves **sliding a small matrix also called "kernel" or "filter" over the image**. This kernel is typically a square matrix with numbers. 
At each position, the kernel multiplies element-wise with the overlapping part of the image, and then adds up all the results to produce a single number. This process is repeated across the entire image and as the filter moves, it detects features such as edges, textures, or shapes at different positions.
Here is a simple illustration:
![Image 9](visuals/1.Mathematics-&-Gradients-everywhere/9.png)  

The mathematical formula is the follwing one
![Image 9.1](visuals/1.Mathematics-&-Gradients-everywhere/9.1.png)  

The PyTorch code to apply this operation to a given image with the same kernel as above can be:

```python


```

I'm sure you're asking yourself, what if we use a different kernel on the same image? 
Well, by applying multiple filters to an image, Convolutional layers can learn to detect various features at different scales and orientations.

For example, a kernel might detect vertical edges by responding strongly to transitions from dark to light or light to dark in a vertical direction. 
After convolution, the resulting image will typically have highlighted features that the kernel was designed to detect.

![Image 10](visuals/1.Mathematics-&-Gradients-everywhere/10.png)  




### Convolution and bias, followed by ReLU

![Image 12](visuals/1.Mathematics-&-Gradients-everywhere/12.png)  



```python


```

![Image 13](visuals/1.Mathematics-&-Gradients-everywhere/13.png)  


### Assembling building blocks: Neural network architecture

![Image 14](visuals/1.Mathematics-&-Gradients-everywhere/14.png)  



```python


```

Once the Neural Network parameters "learned", we can test it:
```python


```

> **How does the Neural Network modify the image layer by layer?**
Well an image is worth thousand words:

![Image 15](visuals/1.Mathematics-&-Gradients-everywhere/15.png)  

Here are the steps summurized:
- 

Using this architecture we got around 96% correct predictions on 10000 images here (may be â‰ˆ99% with other CNNs). 

### Summury

![Image 16](visuals/1.Mathematics-&-Gradients-everywhere/16.png)  

CNN learning: Optimization of [convolution filters], [bias] and [last layer matrices 
layers] so that they lead to the best possible predictions on a training set. 
- Filters highlight relevant properties in the image 
- Mixing different channels can create advanced representations of information 
- Using biases, ReLU functions can remove unnecessary information 
- Max-pooling selects the most relevant information from each image region



# II- Gradient Descent

Problematic: Let's say, we know the students' grades in the 2022-2023 school year, as well as their marks in an end-of-year competition. 
We would like to predict the grades of the students in the class of 2023-2024 based on their grades during the year

![Image 17](visuals/1.Mathematics-&-Gradients-everywhere/17.png)  

- Let's use a very simple model: linear regression with parameters O = {w_0, w_1,..., w_p}!


# III- Parameters learning of a Neural Network
